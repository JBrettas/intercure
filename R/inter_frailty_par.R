library(doSNOW)
library(foreach)
library(Rmpfr)
library(MASS)
library(Matrix)
library(MLEcens)
library(survival)
library(numDeriv)
library(snow)
library(dplyr)

#' Generates a dataset using frailty cure rate model.
#'
#' \code{sim_frailty_data} returns a dataset generated by the cure rate frailty
#' model.
#' @param N Size of the sample to be generated.
#' @param theta Three parameters associated with the cure linear predictor.
#' @param beta Two parameters associated with the hazard function.
#' @param A A positive number representing a fixed right censoring.
#' @param B A positive number which multiplies an uniform random variable,
#'   defining another right censoring case.
#' @param prob Probability that individual presents treatment T1 (baseline is
#'   T0).
#' @return A generated dataset with columns: \code{Z}, the actual event time;
#'   \code{L}, the leftmost limit of the censored interval; \code{R}, the
#'   rightmost limit of the censored interval; \code{delta}, the failure
#'   indicator; \code{x1}, the treatment covariate assuming 1 with probability
#'   \code{prob} and 0 otherwise; \code{xi2}, second variable generated by a
#'   standard normal distribution.
#' @examples
#' sim_frailty_data(20)
sim_frailty_data <- function(N, theta = c(-1,1,0), beta = c(0,0.5), A = 5, B = 15, prob = 0.5) {
  u <- runif(N)
  a <- runif(N)
  C <- cbind(A,a*B)
  C <- C[,1] * (C[,1] <= C[,2]) + C[,2] * (C[,1] > C[,2])
  intercept <- 1
  xi1 <- rbinom(N,1,prob)
  xi2 <- rnorm(N)
  cov_theta <- data.frame(intercept, xi1, xi2)
  cov_beta <- data.frame(xi1, xi2)
  eta <- exp(as.vector(theta %*% t(cov_theta)))
  K_vector <- rpois(N, eta/2)
  U_vector <- K_vector * NA
  for(i in 1:length(K_vector)) {
    if(K_vector[i]==0) U_vector[i]=0
    else{
      U_vector[i] <- 0
      for(j in 1:K_vector[i]) U_vector[i] <- U_vector[i] + rchisq(1,2,ncp=0)
    }
  }
  beta_x <- as.vector(beta %*% t(cov_beta))
  exp_pred_beta <- exp(beta_x)
  num <- -2 * log(1-u)
  den <- U_vector * exp_pred_beta
  tempos <- ifelse(U_vector != 0, sqrt(num/den), Inf)
  Z <- ifelse(tempos < C, tempos, C)
  delta <- ifelse(tempos < C, 1, 0)
  L = R = Z * NA
  for(i in 1:N){
    if(delta[i] == 0) {
      L[i] <- Z[i]
      R[i] <- Inf
    }
    else {
      L[i] <- 0
      add <- runif(1, 0.1, 0.5)
      R[i] <- add
      check <- (L[i] <= Z[i] & Z[i] < R[i])
      while(!check) {
        L[i] <- L[i] + add
        add <- runif(1, 0.1, 0.5)
        R[i] <- R[i] + add
        check <- (L[i] <= Z[i] & Z[i] < R[i])
      }
    }
  }
  dados <- data.frame(Z, L, R, delta, xi1, xi2)
  return(dados)
}

# Fast search for interval containing x
findInterval2 <- function(x,v) {
  n = length(v)
  if (x<v[1])
    return (0)
  if (x>=v[n])
    return (n)
  i=1
  k=n
  while({j = (k-i) %/% 2 + i; !(v[j] <= x && x < v[j+1])}) {
    if (x < v[j])
      k = j
    else
      i = j+1
  }
  return (j)
}


#Creates a table with time and respective Nelson-Aalen estimates for Cum. Hazard Function
#including latent variables u_i
Nelson_Aalen_Table <- function(dataSet,eventTimes,delta,beta,covariates,u){
  factors <- u*exp(beta%*%t(dataSet[,covariates]))
  eventTimes_relev <- unique(sort(eventTimes[delta==1]))
  parc_f <- function(t) (sum(eventTimes[delta==1]==t)/sum(factors[eventTimes>=t]))
  parcels <- sapply(eventTimes_relev  , parc_f  )
  cum_haz <- cumsum(parcels)
  c_haz_f <- data.frame(eventTimes_relev,cum_haz)
  colnames(c_haz_f) <- c("time","hazard")
  return(c_haz_f)
}

#Auxiliar function to Nelson-Aalen estimates table
aux_naalen <- function(tempos, naalen_f, cl=NULL) {
  z <- unique(sort(tempos))
  vector_naalen <- parSapply(cl, z, naalen_f)
  pieceAalen <- data.frame(z, vector_naalen)
  colnames(pieceAalen) <- c("time", "hazard")
  return(pieceAalen)
}


# Survival Function given x(0) , x(1), Theta, Beta
# Note: Precision problems may occur on the linear predictors
surv_lam <- function(t, xi_0, xi_1, theta, beta, nelson_aalen_function){
  exp(-(exp(theta %*% (xi_0)) / 2) * ((1 - (1 / (1 + 2 * nelson_aalen_function(t)*exp(beta %*% (xi_1)))))))
}

# Inverse F function conditioned on Li and Ri (Method 2 of Lam et al 2007)
inverse_lam_f <- function(w, Li, Ri, xi_0, xi_1, theta, beta, naalen_original){
  S_L <- surv_lam(Li, xi_0, xi_1, theta, beta, naalen_original)
  S_R <- surv_lam(Ri, xi_0, xi_1, theta, beta, naalen_original)
  if (S_L == S_R) {
    cat(" Warning! Precision problem on linear predictor. (S_L = S_R) ")
    stop
  }
  k <- (1 - w) * S_L + w * S_R
  k_linha <- -2 * log(k) / ((exp(theta %*% (xi_0)) + 2 * log(k)) * 2 * exp(beta %*% (xi_1)))
  if (k_linha == 0) {
    cat(" Warning. Division by 0 due to precision problems for w= ", w)
    stop
  }
  nAalen_Li <- naalen_original(Li)
  nAalen_Ri <- naalen_original(Ri)
  a <- (nAalen_Ri * Li - nAalen_Li * Ri) / (nAalen_Ri - nAalen_Li)
  b <- (Ri - Li) / (nAalen_Ri - nAalen_Li)
  a + k_linha * b
}

# Generates n observations y using the inverse transformation
gera_yh <- function(data_set, L, R, delta, cov_theta, cov_beta, theta, beta, naalen_original) {
  tam <- length(L)
  new_y <- rep(NA, tam)
  intercepto <- 1
  X_theta <- tbl_df(data_set[,cov_theta])
  X_beta <- tbl_df(data_set[,cov_beta])
  Uh <- runif(tam)
  for (i in 1:tam) {
    if ((delta[i] == 0) | (L[i] == R[i])) {new_y[i] <- L[i]}
    else {
      xi_0 <- as.numeric(cbind(intercepto, X_theta[i,]))
      xi_1 <- as.numeric(X_beta[i,])
      new_y[i] <- inverse_lam_f(Uh[i], L[i], R[i], xi_0, xi_1, theta, beta, naalen_original)
    }
  }
  return(new_y)
}

# Generates latent variables k for n observations
gera_kh <- function(y_h, data_set, delta, cov_theta, cov_beta, theta, beta, nelson_aalen_f) {
  k_h <- y_h * NA
  intercepto <- 1
  xi_0 <- cbind(intercepto, data_set[,cov_theta])
  xi_1 <- data_set[,cov_beta]
  num <- exp(as.vector(theta %*% t(xi_0)))
  den <- 2 + 4 * sapply(y_h, nelson_aalen_f) * exp(as.vector(beta %*% t(xi_1)))
  k_h <- rpois(length(num), num / den) + delta
  return(k_h)
}

# Generates latent variables u for n observations
gera_uh <- function(y_h, k_h, data_set, R, delta, cov_beta, beta, nelson_aalen_f) {
  u_h <- y_h * NA
  r_estrela <- max(R[delta == 1])
  xi_1 <- data_set[,cov_beta]
  alpha_gamma <- k_h + delta
  beta_gamma <- 1 / (0.5 + sapply(y_h, nelson_aalen_f) * exp(as.vector(beta %*% t(xi_1))))
  cond_u <- (k_h == 0 | y_h > r_estrela)
  u_h <- ifelse(cond_u, 0, rgamma(length(alpha_gamma), alpha_gamma, scale=beta_gamma))
  return(u_h)
}


# Returns covariance matrix from (5) of (Lam et al 2007)
var_matrix <- function(sum_var, alpha_matrix) {
  M <- nrow(alpha_matrix)
  alpha_j <- colMeans(alpha_matrix)
  matriz_soma_2 <- diag(ncol(alpha_matrix)) * 0
  for (h in 1:M) {
    matriz_soma_2 <- matriz_soma_2 + ((alpha_matrix[h,] - alpha_j) %*% t(alpha_matrix[h,] - alpha_j)) / (M - 1)
  }
  sigma_alpha <- (sum_var / M) + (1 + 1 / M) * matriz_soma_2
  return(sigma_alpha)
}

#Convergence criteria returning TRUE of FALSE
convergencia <- function(alpha_new, alpha_old, tol = 0.001) {
  conv <- FALSE
  new_val <- c(alpha_new)
  old_val <- c(alpha_old)
  max_error <- max(abs(new_val - old_val))
  if (max_error < tol) conv <- TRUE
  return(conv)
}


#' Fits cure rate frailty model for interval censored data.
#'
#' \code{inter_frailty} returns a list with the estimated parameters \code{par}
#' and their covariance matrix \code{mcov}. The list also contains the cure rate
#' covariance estimates \code{mcov.cura} for cure rate part only and a dummy
#' variable \code{StopC} assuming 0 if algorithm converged and 1 if a stop
#' criteria ended the process.
#'
#' @param data_set Dataset used to fit the model.
#' @param L Vector containing the last check times before event.
#' @param R Vector containing the first check times after event.
#' @param delta Flag vector indicating failure inside interval.
#' @param cov_theta String vector containing the column names to be used on the
#'   cure rate predictor.
#' @param cov_beta String vector containing the column names to be used on the
#'   predictor associated with the hazard function.
#' @param M Number of replicates generated by each iteration on the ANDA
#'   (Asymptotic Normal Data Augmentation) algorithm.
#' @param b Numeric for tolerance of convergence.
#' @param N_INT_MAX Maximum number of algorithm's iterations without the burn
#'   in.
#' @param ncores Number of cores used for parallel process.
#' @param burn_in Number of burn in iterations.
#' @return The \code{inter_frailty} function returns an list containing the following outputs.
#' @return \code{par} estimates of theta and beta parameters.
#' @return \code{mcov} estimates for the covariance matrix of theta and beta
#'   parameters.
#' @return \code{mcov.cura} estimates for the covariance matrix associated with the
#'   cure rate part.
#' @return \code{StopC} stop criteria indicator assuming 1 when process is stopped for
#'   a non-convergence criteria. Assumes 0 when convergence is reached.
#' @examples
#' sample_set <- sim_frailty_data(100)
#' inter_frailty(sample_set, sample_set$L, sample_set$R, sample_set$delta, c("xi1","xi2"), c("xi1","xi2"), M = 10)
inter_frailty <- function(data_set, L, R, delta, cov_theta, cov_beta, M, b=0.001, N_INT_MAX=100, NAME_DIF="", ncores=1, burn_in=30) {
  # Registering clusters for parallel computing
  cl <- makeCluster(ncores, type="SOCK")
  registerDoSNOW(cl)
  data_set <- tbl_df(data_set)

  # Defining output variables
  est_file_name <- paste("LAM_Estimates", NAME_DIF, ".txt", sep="")
  var_file_name <- paste("LAM_Variances", NAME_DIF, ".txt", sep="")
  fileConn <- file(est_file_name, "w")

  # Initial values for y
  y_nxM = k_nxM = u_nxM = matrix(NA, nrow=M, ncol = nrow(data_set))
  for(i in 1:M) y_nxM[i,] <- ifelse(delta == 1, (L + R) / 2, L)

  # Initial values for the parameters
  compr_theta <- 1 + length(cov_theta); compr_beta <- length(cov_beta)
  compr_alpha <- compr_theta + compr_beta
  a_M <- matrix(NA, nrow=M, ncol=compr_alpha)
  rotulos <- c("intercepto", cov_theta, cov_beta)
  colnames(a_M) <- rotulos
  theta_M <- matrix(a_M[,1:compr_theta], nrow=M)
  colnames(theta_M) <- rotulos[1:compr_theta]
  beta_M <- matrix(a_M[,(compr_theta + 1):compr_alpha], nrow=M)
  colnames(beta_M) <- rotulos[(compr_theta + 1):compr_alpha]
  alpha <- c(1:compr_alpha) * 0
  sigma_alpha <- b * diag(compr_alpha); beta <- alpha[(compr_theta + 1):compr_alpha]

  # Initial values for latent vector u
  u <- delta

  # Initial Nelson-Aalen estimator
  Vetores_NAalen <- Nelson_Aalen_Table(data_set, y_nxM[1,], delta, beta, cov_beta, u)
  NAalen_MEDIA <- stepfun(Vetores_NAalen$time, c(0,Vetores_NAalen$hazard))

  # Initializing convergence criteria and parameters
  conv <- FALSE; n <- 0
  a_M_NEW <- a_M

  # Iterative process (with parallel computing)
  while(!conv | n <= burn_in) {
    cat("ITER#", (n + 1))
    tempoITER <- system.time({
    list_reg = foreach(icount(M), .packages=c("dplyr","MASS","Matrix","survival"), .export=c("surv_lam","inverse_lam_f", "gera_yh", "gera_kh", "gera_uh"), .inorder=F) %dopar% {
      a_M <- mvrnorm(n=1, alpha, sigma_alpha)
      theta_M <- a_M[1:compr_theta]; beta_M <- a_M[(compr_theta + 1):compr_alpha]
      y <- gera_yh(data_set, L, R, delta, cov_theta, cov_beta, as.numeric(theta_M), as.numeric(beta_M), NAalen_MEDIA)
      k <- gera_kh(y, data_set, delta, cov_theta, cov_beta, theta_M, beta_M, NAalen_MEDIA)
      u <- gera_uh(y, k, data_set, R, delta, cov_beta, beta_M, NAalen_MEDIA)

      # Poisson Regression for Theta
      o_set <- k * 0 - log(2)
      expression_theta <- paste("data_set$", cov_theta[1:length(cov_theta)], sep = "", collapse="+")
      eq_theta <- paste("fit_theta <- glm(k~", expression_theta, "+offset(o_set),family=poisson)")
      eval(parse(text = eq_theta))

      # Cox Regression for Beta
      expression_beta <- paste("data_set$", cov_beta[1:length(cov_beta)] ,sep = "", collapse="+")
      eq_beta <- paste("fit_beta <- coxph(Surv(y,delta)~", expression_beta," + offset(ifelse(log(u)==-Inf,-200,log(u))),method='breslow')")
      eval(parse(text = eq_beta))

      # Outputs of Parallel Computing
      out <- list(fit_theta$coef, vcov(fit_theta), fit_beta$coef, vcov(fit_beta), y, u)
      out
    }

    # Allocating M new and auxiliary parameter vectors
    SUM_VAR_THETA = SUM_VAR_BETA = 0
    for (h in 1:M) {
      SUM_VAR_THETA = SUM_VAR_THETA + list_reg[[h]][[2]]; SUM_VAR_BETA = SUM_VAR_BETA + list_reg[[h]][[4]]
      a_M_NEW[h,1:compr_theta] = list_reg[[h]][[1]]
      a_M_NEW[h,(compr_theta + 1):compr_alpha] = list_reg[[h]][[3]]
      y_nxM[h,] = list_reg[[h]][[5]]
      u_nxM[h,] = list_reg[[h]][[6]]
    }

    # Matrix of the M beta vectors
    beta_M <- matrix(a_M_NEW[,(compr_theta + 1):compr_alpha], nrow=M)

    # Obtaining new Nelson-Aalen estimator for Cum. Hazard function
    step_list <- foreach(h=1:M, .export="Nelson_Aalen_Table", .inorder=F) %dopar% {
      V_NAalen <- Nelson_Aalen_Table(data_set, y_nxM[h,], delta, beta_M[h,], cov_beta, u_nxM[h,])
      step_list <- stepfun(V_NAalen$time, c(0, V_NAalen$hazard))
      step_list
    }
    expression <- paste("step_list[[", 1:M,"]](x)", sep = "", collapse = "+")
    eq4 <- paste("NAalen_MEDIA <- function(x) (",expression,")/", M)
    eval(parse(text = eq4))

    # Creating new times/survival table and a more efficient estimator
    V_NAalen <- aux_naalen(sort(y_nxM[,delta==1]), NAalen_MEDIA, cl)
    NAalen_MEDIAnew <- stepfun(V_NAalen$time, c(0, V_NAalen$hazard))

    #Calculating the new covariance matrix
    SUM_VAR <- as.matrix(bdiag(list(SUM_VAR_THETA, SUM_VAR_BETA)))
    VAR <- var_matrix(SUM_VAR, a_M_NEW)
    sigma_alpha <- VAR

    #New vector of estimates
    alpha_new <- colMeans(a_M_NEW)
    })
    print(tempoITER)

    #Checking convergence
    conv <- convergencia(alpha_new,alpha)

    #Setting new alpha as old one for iteractive process
    alpha <- alpha_new

    #Writing alpha values
    write(as.vector(alpha), file=fileConn, append=T, sep=" ")
    write.table(VAR, file=var_file_name, row.names=FALSE, col.names=FALSE)

    #Setting new baseline cum. hazard estimator as old one for iteractive process
    NAalen_MEDIA <- NAalen_MEDIAnew

    #Updating the iteration counter
    n <- n + 1

    #Checking if iteration counter reached N_INT_MAX
    if (n == (N_INT_MAX + burn_in)) {
      write("Warning: Iteration Number achieved but convergence criteria not met.", file=fileConn, append=T, sep=" ")
      cat("Warning: Convergence criteria not met. Estimates given for N_INT_MAX=", N_INT_MAX)
      break
    }
  }
  #Kills the parallel proccess
  cPar <- as.numeric(n == (N_INT_MAX + burn_in))
  stopCluster(cl)
  alphaList <- list(par = alpha, mcov = VAR, mcov.cura = VAR[1:(1+length(cov_theta)), 1:(1 + length(cov_theta))], StopC = cPar)
  close(fileConn)
  return(alphaList)
}

